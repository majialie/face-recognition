{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/cqm/datasets/celeba/index.txt') as input_file:\n",
    "    labels = []\n",
    "    images = []\n",
    "    for line in input_file.readlines():\n",
    "        line = line.strip('\\n').split(\" \")\n",
    "        imgid = line[0]\n",
    "        for i in range(1, len(line)):\n",
    "            labels.append(imgid)\n",
    "            images.append(line[i])\n",
    "print('images and labels prepared')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print labels[400*128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/cqm/datasets/celeba/index.txt') as input_file:\n",
    "    labels = []\n",
    "    images = []\n",
    "    labels_b = []\n",
    "    images_b = []\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    for line in input_file.readlines():\n",
    "        line = line.strip('\\n').split(\" \")\n",
    "        imgid = line[0]\n",
    "        for i in range(1, len(line)):\n",
    "            if count1 >= 128:\n",
    "                count1 = 0 \n",
    "            if count1 < 64 and count2%2 == 0:\n",
    "                labels.append(imgid)\n",
    "                images.append(line[i])\n",
    "                count1 += 1\n",
    "                count2 += 1\n",
    "            elif count1 < 64 and count2%2 == 1:\n",
    "                labels_b.append(imgid)\n",
    "                images_b.append(line[i])\n",
    "                count1 += 1\n",
    "                count2 += 1\n",
    "            elif count1 < 96:\n",
    "                labels.append(imgid)\n",
    "                images.append(line[i])\n",
    "                count1 += 1\n",
    "                count2 += 1 \n",
    "            elif count1 < 128:\n",
    "                labels_b.append(imgid)\n",
    "                images_b.append(line[i])\n",
    "                count1 += 1\n",
    "                count2 += 1\n",
    "print('images and labels prepared')\n",
    "print len(labels), len(images),len(labels_b), len(images_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print labels[400*64]\n",
    "3063 016870.jpg 073859.jpg 158145.jpg\n",
    "3064 165103.jpg 169435.jpg 172406.jpg 174072.jpg 178397.jpg 178857.jpg\n",
    "3065 018685.jpg 020421.jpg 030522.jpg 035702.jpg 039798.jpg 102418.jpg 107034.jpg 128058.jpg 156975.jpg 158330.jpg\n",
    "3066 004580.jpg 006779.jpg 032653.jpg 032665.jpg 039101.jpg 047802.jpg 052432.jpg 055288.jpg 057025.jpg 057347.jpg 062589.jpg 065327.jpg 069574.jpg 108118.jpg 113621.jpg 132199.jpg 133261.jpg 135264.jpg 135780.jpg 147598.jpg 147706.jpg 155663.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "x = tf.placeholder(tf.float32,[None, 64, 64, 3])\n",
    "y = tf.placeholder(tf.float32, [None])\n",
    "#y_is_same = tf.placeholder(tf.float32, [None])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "with open('/home/cqm/datasets/celeba/index.txt') as input_file:\n",
    "    person_index = []\n",
    "    for line in input_file.readlines():\n",
    "        line = line.strip('\\n').split(\" \")\n",
    "        person_index.append(line[1:])\n",
    "    print \"image and label data loaded\"\n",
    "    \n",
    "path = '/home/cqm/datasets/celeba/img/'  \n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, w):\n",
    "    return tf.nn.conv2d(x, w, strides = [1, 1, 1, 1], padding = \"SAME\")\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")\n",
    "\n",
    "def network(x):\n",
    "    w_conv1 = weight_variable([3, 3, 3, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "    h_conv1 = tf.nn.relu(conv2d(x, w_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1) #32*32*32\n",
    "\n",
    "    w_conv2 = weight_variable([3, 3, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2) #64*16*16\n",
    "\n",
    "    w_conv3 = weight_variable([3, 3, 64, 128])\n",
    "    b_conv3 = bias_variable([128])\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_pool2, w_conv3) + b_conv3)\n",
    "    h_pool3 = max_pool_2x2(h_conv3) #128*8*8\n",
    "\n",
    "\n",
    "    h_pool3_reshape = tf.reshape(h_pool3, [-1, 8*16*64])\n",
    "\n",
    "    w_fc1 = weight_variable([8*16*64, 512])\n",
    "    b_fc1 = bias_variable([512])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool3_reshape, w_fc1) + b_fc1)\n",
    "    return h_fc1\n",
    "\n",
    "def contrastive_loss(y, d):\n",
    "    tmp = y*tf.square(d)\n",
    "    tmp2 = (1-y)*tf.square(tf.maximum((20-d), 0))\n",
    "    return tf.reduce_sum(tmp+tmp2)/batch_size\n",
    "\n",
    "def getface(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    face_cascade = cv2.CascadeClassifier('/home/cqm/datasets/celeba/haarcascade_frontalface_default.xml')\n",
    "    face = face_cascade.detectMultiScale(gray, 1.3, 4)\n",
    "    if len(face) > 0:\n",
    "        for x, y, w, h in face:\n",
    "            continue\n",
    "        img = img[y:y+h, x:x+w]\n",
    "    img = cv2.resize(img, (64, 64))/127.5-1.0\n",
    "    return img\n",
    "        \n",
    "\n",
    "out = network(x)\n",
    "d = tf.reduce_sum((out[:batch_size] - out[batch_size:])**2+1e-5, axis = 1)**0.5\n",
    "\n",
    "loss = contrastive_loss(y, d)\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "#correct_prediction = tf.equal(tf.argmax(y_out,1), tf.argmax(y_,1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#saver = tf.train.Saver(max_to_keep = 1)\n",
    "\n",
    "for i in range(50000):\n",
    "    pos_pair_list1 = []\n",
    "    pos_pair_list2 = []\n",
    "    while len(pos_pair_list1) < 32:\n",
    "        id = np.random.choice(len(person_index),1,replace=False)[0]\n",
    "        if len(person_index[id]) >= 2:\n",
    "            pair_id = np.random.choice(len(person_index[id]),2,replace=False)\n",
    "            pos_pair_list1.append(person_index[id][pair_id[0]])\n",
    "            pos_pair_list2.append(person_index[id][pair_id[1]])\n",
    "\n",
    "    neg_pair_list1 = []\n",
    "    neg_pair_list2 = []\n",
    "    while len(neg_pair_list1) < 32:\n",
    "        id = np.random.choice(len(person_index),2,replace=False)\n",
    "        neg_id1 = np.random.choice(len(person_index[id[0]]),1,replace=False)[0]\n",
    "        neg_id2 = np.random.choice(len(person_index[id[1]]),1,replace=False)[0]\n",
    "        neg_pair_list1.append(person_index[id[0]][neg_id1])\n",
    "        neg_pair_list2.append(person_index[id[1]][neg_id2])\n",
    "    \n",
    "    x_imgs = []\n",
    "    y_labels = [1] * 32 + [0] * 32\n",
    "    for img_name in pos_pair_list1:\n",
    "        img = cv2.imread(path + img_name)\n",
    "        #img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        #img = cv2.resize(img, (64, 64))/127.5 - 1.0\n",
    "        img = getface(img)\n",
    "        x_imgs.append(img)\n",
    "    for img_name in neg_pair_list1:\n",
    "        img = cv2.imread(path + img_name)\n",
    "        #img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        #img = cv2.resize(img, (64, 64))/127.5 - 1.0\n",
    "        img = getface(img)\n",
    "        x_imgs.append(img)\n",
    "    for img_name in pos_pair_list2:\n",
    "        img = cv2.imread(path + img_name)\n",
    "        #img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        #img = cv2.resize(img, (64, 64))/127.5 - 1.0\n",
    "        img = getface(img)\n",
    "        x_imgs.append(img)\n",
    "    for img_name in neg_pair_list2:\n",
    "        img = cv2.imread(path + img_name)        \n",
    "        #img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        #img = cv2.resize(img, (64, 64))/127.5 - 1.0\n",
    "        img = getface(img)\n",
    "        x_imgs.append(img)\n",
    "    x_imgs = np.array(x_imgs, dtype=np.float32)\n",
    "\n",
    "    if i%10 == 0:\n",
    "        train_loss = loss.eval(feed_dict={x:x_imgs,y:y_labels})\n",
    "        print('step %d , training loss %f'%(i, train_loss))\n",
    "    train_step.run(feed_dict={x:x_imgs,y:y_labels})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "count = 1\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "\n",
    "num = 407\n",
    "\n",
    "for img_name1, img_name2 in zip(images[num*batch_size:(num+1)*batch_size],images_b[num*batch_size:(num+1)*batch_size]):\n",
    "    img1 = cv2.imread(path + img_name1)\n",
    "    img1 = cv2.resize(img1, (64, 64))\n",
    "    img2 = cv2.imread(path + img_name2)\n",
    "    img2 = cv2.resize(img2, (64, 64))\n",
    "    plt.subplot(16,8,count)\n",
    "    plt.imshow(img1[:,:,::-1])\n",
    "    count += 1\n",
    "    plt.subplot(16,8,count)\n",
    "    plt.imshow(img2[:,:,::-1])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "zlc1 = cv2.imread(path+images[407*batch_size+10])\n",
    "\n",
    "plt.imshow(zlc1)\n",
    "\n",
    "zlc2 = cv2.imread(path+images_b[407*batch_size+10])\n",
    "\n",
    "plt.imshow(zlc2)\n",
    "\n",
    "images_b[num*batch_size+10]\n",
    "\n",
    "images[num*batch_size+10]\n",
    "\n",
    "zlc1 - zlc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "path = 'test/3/'\n",
    "dir_list = os.listdir(path)\n",
    "\n",
    "for imgid in dir_list:\n",
    "    img = cv2.imread(path + imgid)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    face_cascade = cv2.CascadeClassifier('/home/cqm/datasets/celeba/haarcascade_frontalface_default.xml')\n",
    "    face = face_cascade.detectMultiScale(gray, 1.3, 4)\n",
    "    print len(face)\n",
    "    if len(face) > 0:\n",
    "        for x, y, w, h in face:\n",
    "            continue\n",
    "        img = img[y:y+h, x:x+w]\n",
    "    img = cv2.resize(img,(64,64))\n",
    "    cv2.imwrite(path+imgid, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "person = cv2.imread('test/409.jpg')\n",
    "gray = cv2.cvtColor(person, cv2.COLOR_BGR2GRAY)\n",
    "face_cascade = cv2.CascadeClassifier('/home/cqm/datasets/celeba/haarcascade_frontalface_default.xml')\n",
    "face = face_cascade.detectMultiScale(gray, 1.3, 4)\n",
    "print len(face)\n",
    "if len(face) > 0:\n",
    "    for x, y, w, h in face:\n",
    "        continue\n",
    "    cv2.rectangle(person, (x, y), (x+w,y+h),(0,255, 0),2)\n",
    "plt.imshow(person[:,:,::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "x = tf.placeholder(tf.float32,[None, 64, 64, 3])\n",
    "y = tf.placeholder(tf.float32, [None])\n",
    "yid = tf.placeholder(tf.float32, [None])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "with open('/home/cqm/datasets/celeba/index.txt') as input_file:\n",
    "    person_index = []\n",
    "    label_index = []\n",
    "    for line in input_file.readlines():\n",
    "        line = line.strip('\\n').split(\" \")\n",
    "        label_index.append(line[0])\n",
    "        person_index.append(line[1:])\n",
    "    print \"image and label data loaded\"\n",
    "    \n",
    "path = '/home/cqm/datasets/celeba/img/'  \n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, w):\n",
    "    return tf.nn.conv2d(x, w, strides = [1, 1, 1, 1], padding = \"SAME\")\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")\n",
    "\n",
    "def network(x):\n",
    "    w_conv1 = weight_variable([3, 3, 3, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "    h_conv1 = tf.nn.relu(conv2d(x, w_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1) #32*32*32\n",
    "\n",
    "    w_conv2 = weight_variable([3, 3, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2) #64*16*16\n",
    "\n",
    "    w_conv3 = weight_variable([3, 3, 64, 128])\n",
    "    b_conv3 = bias_variable([128])\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_pool2, w_conv3) + b_conv3)\n",
    "    h_pool3 = max_pool_2x2(h_conv3) #128*8*8\n",
    "\n",
    "\n",
    "    h_pool3_reshape = tf.reshape(h_pool3, [-1, 8*16*64])\n",
    "\n",
    "    w_fc1 = weight_variable([8*16*64, 512])\n",
    "    b_fc1 = bias_variable([512])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool3_reshape, w_fc1) + b_fc1)\n",
    "    return h_fc1\n",
    "\n",
    "def contrastive_loss(y, d):\n",
    "    tmp = y*tf.square(d)\n",
    "    tmp2 = (1-y)*tf.square(tf.maximum((20-d), 0))\n",
    "    return tf.reduce_sum(tmp+tmp2)/batch_size\n",
    "        \n",
    "\n",
    "x_out = network(x)\n",
    "d_pos = tf.reduce_sum(tf.square(x_out[:64] - x_out[64:128]), 1)\n",
    "d_neg = tf.reduce_sum(tf.square(x_out[:64] - x_out[128:]), 1)\n",
    "loss = tf.reduce_mean(tf.maximum(0.0, 50 + d_pos - d_neg))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "#correct_prediction = tf.equal(tf.argmax(y_out,1), tf.argmax(y_,1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(max_to_keep = 1)\n",
    "\n",
    "for i in range(20000):\n",
    "    pos_pair_list1 = []\n",
    "    pos_pair_list2 = []\n",
    "    neg_pair_list = []\n",
    "    while len(pos_pair_list1) < 64:\n",
    "        id = np.random.choice(len(person_index),2,replace=False)\n",
    "        if len(person_index[id[0]]) >= 2:\n",
    "            pair_id = np.random.choice(len(person_index[id[0]]),2,replace=False)\n",
    "            neg_id = np.random.choice(len(person_index[id[1]]),1,replace=False)[0]\n",
    "            pos_pair_list1.append(person_index[id[0]][pair_id[0]])\n",
    "            pos_pair_list2.append(person_index[id[0]][pair_id[1]])\n",
    "            neg_pair_list.append(person_index[id[1]][neg_id])\n",
    "    x_imgs = []\n",
    "    for img_name in pos_pair_list1:\n",
    "        img = cv2.imread(path + img_name)\n",
    "        img = cv2.resize(img, (64, 64))/127.5 - 1.0\n",
    "        #img = getface(img)\n",
    "        x_imgs.append(img)\n",
    "    for img_name in pos_pair_list2:\n",
    "        img = cv2.imread(path + img_name)\n",
    "        img = cv2.resize(img, (64, 64))/127.5 - 1.0\n",
    "        #img = getface(img)\n",
    "        x_imgs.append(img)\n",
    "    for img_name in neg_pair_list:\n",
    "        img = cv2.imread(path + img_name)\n",
    "        img = cv2.resize(img, (64, 64))/127.5 - 1.0\n",
    "        #img = getface(img)\n",
    "        x_imgs.append(img)\n",
    "    x_imgs = np.array(x_imgs, dtype=np.float32)\n",
    "\n",
    "    if i%10 == 0:\n",
    "        train_loss = loss.eval(feed_dict={x:x_imgs})\n",
    "        print('step %d , training loss %f'%(i, train_loss))\n",
    "    train_step.run(feed_dict={x:x_imgs})\n",
    "    \n",
    "    \n",
    "saver.save(sess,'model/face2.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "test1 = cv2.imread('testimg/eita1.jpg')\n",
    "test1 = cv2.resize(test1,(64, 64))\n",
    "test2 = cv2.imread('testimg/eita2.jpg')\n",
    "test2 = cv2.resize(test2,(64, 64))\n",
    "test3 = cv2.imread('testimg/long1.jpg')\n",
    "test3 = cv2.resize(test3,(64, 64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = test1.astype('float32')[None]/127.5 - 1.0\n",
    "test2 = test2.astype('float32')[None]/127.5 - 1.0\n",
    "test3 = test3.astype('float32')[None]/127.5 - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = x_out.eval(feed_dict={x:test1})\n",
    "res2 = x_out.eval(feed_dict={x:test2})\n",
    "res3 = x_out.eval(feed_dict={x:test3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = ((res1-res2)**2).sum()\n",
    "d2 = ((res1-res3)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print d1\n",
    "print d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
