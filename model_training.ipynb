{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 , training loss 168.181427\n",
      "step 10 , training loss 138.362961\n",
      "step 20 , training loss 154.492706\n",
      "step 30 , training loss 120.378342\n",
      "step 40 , training loss 111.209435\n",
      "step 50 , training loss 91.913033\n",
      "step 60 , training loss 94.863815\n",
      "step 70 , training loss 107.805054\n",
      "step 80 , training loss 107.882843\n",
      "step 90 , training loss 84.134720\n",
      "step 100 , training loss 102.999771\n",
      "step 110 , training loss 101.733337\n",
      "step 120 , training loss 72.625153\n",
      "step 130 , training loss 90.436523\n",
      "step 140 , training loss 91.049835\n",
      "step 150 , training loss 107.345474\n",
      "step 160 , training loss 95.233780\n",
      "step 170 , training loss 77.854141\n",
      "step 180 , training loss 102.719269\n",
      "step 190 , training loss 78.661095\n",
      "step 200 , training loss 84.749496\n",
      "step 210 , training loss 83.661148\n",
      "step 220 , training loss 80.462875\n",
      "step 230 , training loss 90.011787\n",
      "step 240 , training loss 99.497734\n",
      "step 250 , training loss 102.271225\n",
      "step 260 , training loss 86.126190\n",
      "step 270 , training loss 78.429474\n",
      "step 280 , training loss 94.470169\n",
      "step 290 , training loss 79.975021\n",
      "step 300 , training loss 83.450729\n",
      "step 310 , training loss 96.598236\n",
      "step 320 , training loss 88.984741\n",
      "step 330 , training loss 86.796898\n",
      "step 340 , training loss 92.023956\n",
      "step 350 , training loss 81.347885\n",
      "step 360 , training loss 85.490486\n",
      "step 370 , training loss 72.184181\n",
      "step 380 , training loss 79.659531\n",
      "step 390 , training loss 89.058281\n",
      "step 400 , training loss 78.626892\n",
      "step 410 , training loss 63.024544\n",
      "step 420 , training loss 84.071014\n",
      "step 430 , training loss 70.243637\n",
      "step 440 , training loss 88.214676\n",
      "step 450 , training loss 80.091255\n",
      "step 460 , training loss 85.432648\n",
      "step 470 , training loss 78.050140\n",
      "step 480 , training loss 79.191650\n",
      "step 490 , training loss 73.689690\n",
      "step 500 , training loss 78.396652\n",
      "step 510 , training loss 85.253693\n",
      "step 520 , training loss 77.635132\n",
      "step 530 , training loss 66.619926\n",
      "step 540 , training loss 76.094582\n",
      "step 550 , training loss 68.361458\n",
      "step 560 , training loss 63.893871\n",
      "step 570 , training loss 72.611923\n",
      "step 580 , training loss 94.787933\n",
      "step 590 , training loss 69.981506\n",
      "step 600 , training loss 79.620964\n",
      "step 610 , training loss 75.666122\n",
      "step 620 , training loss 68.334236\n",
      "step 630 , training loss 76.982651\n",
      "step 640 , training loss 74.058136\n",
      "step 650 , training loss 63.855865\n",
      "step 660 , training loss 90.091293\n",
      "step 670 , training loss 64.953476\n",
      "step 680 , training loss 74.198120\n",
      "step 690 , training loss 70.857147\n",
      "step 700 , training loss 95.217484\n",
      "step 710 , training loss 59.111633\n",
      "step 720 , training loss 75.319267\n",
      "step 730 , training loss 66.385010\n",
      "step 740 , training loss 69.272049\n",
      "step 750 , training loss 65.091461\n",
      "step 760 , training loss 65.999405\n",
      "step 770 , training loss 74.859657\n",
      "step 780 , training loss 62.370995\n",
      "step 790 , training loss 72.458008\n",
      "step 800 , training loss 79.976685\n",
      "step 810 , training loss 78.695801\n",
      "step 820 , training loss 57.439880\n",
      "step 830 , training loss 71.961838\n",
      "step 840 , training loss 71.774460\n",
      "step 850 , training loss 66.428360\n",
      "step 860 , training loss 56.181725\n",
      "step 870 , training loss 73.242249\n",
      "step 880 , training loss 63.405525\n",
      "step 890 , training loss 60.020790\n",
      "step 900 , training loss 62.864887\n",
      "step 910 , training loss 51.249664\n",
      "step 920 , training loss 72.099258\n",
      "step 930 , training loss 73.447540\n",
      "step 940 , training loss 60.459129\n",
      "step 950 , training loss 70.979095\n",
      "step 960 , training loss 55.101837\n",
      "step 970 , training loss 84.928253\n",
      "step 980 , training loss 56.467545\n",
      "step 990 , training loss 63.765793\n",
      "step 1000 , training loss 60.089630\n",
      "step 1010 , training loss 70.550980\n",
      "step 1020 , training loss 45.324558\n",
      "step 1030 , training loss 55.044010\n",
      "step 1040 , training loss 66.302284\n",
      "step 1050 , training loss 56.230854\n",
      "step 1060 , training loss 72.332886\n",
      "step 1070 , training loss 68.225838\n",
      "step 1080 , training loss 63.355125\n",
      "step 1090 , training loss 59.743244\n",
      "step 1100 , training loss 64.955414\n",
      "step 1110 , training loss 61.769897\n",
      "step 1120 , training loss 65.137512\n",
      "step 1130 , training loss 65.174316\n",
      "step 1140 , training loss 51.510941\n",
      "step 1150 , training loss 54.566566\n",
      "step 1160 , training loss 62.747276\n",
      "step 1170 , training loss 72.794258\n",
      "step 1180 , training loss 62.941467\n",
      "step 1190 , training loss 57.234001\n",
      "step 1200 , training loss 85.568314\n",
      "step 1210 , training loss 78.429108\n",
      "step 1220 , training loss 58.506817\n",
      "step 1230 , training loss 46.622196\n",
      "step 1240 , training loss 77.571365\n",
      "step 1250 , training loss 51.948437\n",
      "step 1260 , training loss 66.853561\n",
      "step 1270 , training loss 72.725021\n",
      "step 1280 , training loss 55.991508\n",
      "step 1290 , training loss 62.411644\n",
      "step 1300 , training loss 62.719185\n",
      "step 1310 , training loss 67.062119\n",
      "step 1320 , training loss 69.033890\n",
      "step 1330 , training loss 63.427902\n",
      "step 1340 , training loss 49.913834\n",
      "step 1350 , training loss 58.467621\n",
      "step 1360 , training loss 54.004089\n",
      "step 1370 , training loss 54.388271\n",
      "step 1380 , training loss 69.526306\n",
      "step 1390 , training loss 79.425049\n",
      "step 1400 , training loss 62.778336\n",
      "step 1410 , training loss 69.345123\n",
      "step 1420 , training loss 77.130798\n",
      "step 1430 , training loss 62.718239\n",
      "step 1440 , training loss 66.251144\n",
      "step 1450 , training loss 66.838699\n",
      "step 1460 , training loss 62.866997\n",
      "step 1470 , training loss 65.117386\n",
      "step 1480 , training loss 73.557999\n",
      "step 1490 , training loss 74.316811\n",
      "step 1500 , training loss 65.559875\n",
      "step 1510 , training loss 74.933502\n",
      "step 1520 , training loss 48.486744\n",
      "step 1530 , training loss 46.426247\n",
      "step 1540 , training loss 45.215706\n",
      "step 1550 , training loss 54.654411\n",
      "step 1560 , training loss 61.883728\n",
      "step 1570 , training loss 60.714680\n",
      "step 1580 , training loss 44.975082\n",
      "step 1590 , training loss 55.002594\n",
      "step 1600 , training loss 56.793392\n",
      "step 1610 , training loss 71.510330\n",
      "step 1620 , training loss 59.239605\n",
      "step 1630 , training loss 57.929977\n",
      "step 1640 , training loss 60.706081\n",
      "step 1650 , training loss 69.570892\n",
      "step 1660 , training loss 53.785168\n",
      "step 1670 , training loss 64.808914\n",
      "step 1680 , training loss 73.605484\n",
      "step 1690 , training loss 78.681046\n",
      "step 1700 , training loss 65.136139\n",
      "step 1710 , training loss 56.880306\n",
      "step 1720 , training loss 45.269291\n",
      "step 1730 , training loss 47.634335\n",
      "step 1740 , training loss 57.292488\n",
      "step 1750 , training loss 53.869358\n",
      "step 1760 , training loss 49.265011\n",
      "step 1770 , training loss 57.699928\n",
      "step 1780 , training loss 54.104408\n",
      "step 1790 , training loss 65.181595\n",
      "step 1800 , training loss 58.519875\n",
      "step 1810 , training loss 55.800659\n",
      "step 1820 , training loss 59.764351\n",
      "step 1830 , training loss 61.834305\n",
      "step 1840 , training loss 68.516586\n",
      "step 1850 , training loss 72.285934\n",
      "step 1860 , training loss 74.012947\n",
      "step 1870 , training loss 68.028748\n",
      "step 1880 , training loss 61.926022\n",
      "step 1890 , training loss 57.962029\n",
      "step 1900 , training loss 67.734589\n",
      "step 1910 , training loss 56.718277\n",
      "step 1920 , training loss 63.111717\n",
      "step 1930 , training loss 48.979973\n",
      "step 1940 , training loss 44.595276\n",
      "step 1950 , training loss 85.521484\n",
      "step 1960 , training loss 63.937775\n",
      "step 1970 , training loss 63.129421\n",
      "step 1980 , training loss 43.841110\n",
      "step 1990 , training loss 59.478600\n",
      "step 2000 , training loss 51.527073\n",
      "step 2010 , training loss 62.298038\n",
      "step 2020 , training loss 60.710514\n",
      "step 2030 , training loss 61.870968\n",
      "step 2040 , training loss 65.457901\n",
      "step 2050 , training loss 58.588600\n",
      "step 2060 , training loss 53.693008\n",
      "step 2070 , training loss 65.777695\n",
      "step 2080 , training loss 66.243034\n",
      "step 2090 , training loss 51.185112\n",
      "step 2100 , training loss 53.037872\n",
      "step 2110 , training loss 62.917114\n",
      "step 2120 , training loss 76.686539\n",
      "step 2130 , training loss 70.784935\n",
      "step 2140 , training loss 56.093224\n",
      "step 2150 , training loss 60.544579\n",
      "step 2160 , training loss 74.556473\n",
      "step 2170 , training loss 51.961452\n",
      "step 2180 , training loss 60.659576\n",
      "step 2190 , training loss 68.525047\n",
      "step 2200 , training loss 53.461681\n",
      "step 2210 , training loss 74.175529\n",
      "step 2220 , training loss 50.764431\n",
      "step 2230 , training loss 56.725250\n",
      "step 2240 , training loss 54.461342\n",
      "step 2250 , training loss 66.770264\n",
      "step 2260 , training loss 38.435608\n",
      "step 2270 , training loss 61.620056\n",
      "step 2280 , training loss 68.862015\n",
      "step 2290 , training loss 57.848465\n",
      "step 2300 , training loss 55.288895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2310 , training loss 58.640518\n",
      "step 2320 , training loss 65.769264\n",
      "step 2330 , training loss 52.335022\n",
      "step 2340 , training loss 56.938129\n",
      "step 2350 , training loss 53.963493\n",
      "step 2360 , training loss 60.343254\n",
      "step 2370 , training loss 54.939705\n",
      "step 2380 , training loss 55.681995\n",
      "step 2390 , training loss 44.607243\n",
      "step 2400 , training loss 75.321793\n",
      "step 2410 , training loss 58.956978\n",
      "step 2420 , training loss 58.449211\n",
      "step 2430 , training loss 55.178696\n",
      "step 2440 , training loss 54.317665\n",
      "step 2450 , training loss 56.763100\n",
      "step 2460 , training loss 38.657558\n",
      "step 2470 , training loss 49.722961\n",
      "step 2480 , training loss 56.467300\n",
      "step 2490 , training loss 62.934036\n",
      "step 2500 , training loss 61.692169\n",
      "step 2510 , training loss 60.657623\n",
      "step 2520 , training loss 64.059448\n",
      "step 2530 , training loss 43.382584\n",
      "step 2540 , training loss 55.755833\n",
      "step 2550 , training loss 55.517200\n",
      "step 2560 , training loss 77.339554\n",
      "step 2570 , training loss 58.337132\n",
      "step 2580 , training loss 55.332279\n",
      "step 2590 , training loss 51.816620\n",
      "step 2600 , training loss 53.972576\n",
      "step 2610 , training loss 64.771286\n",
      "step 2620 , training loss 45.590515\n",
      "step 2630 , training loss 41.749954\n",
      "step 2640 , training loss 68.098328\n",
      "step 2650 , training loss 61.696693\n",
      "step 2660 , training loss 52.447365\n",
      "step 2670 , training loss 56.364906\n",
      "step 2680 , training loss 62.468697\n",
      "step 2690 , training loss 50.847237\n",
      "step 2700 , training loss 42.971161\n",
      "step 2710 , training loss 56.332893\n",
      "step 2720 , training loss 60.615105\n",
      "step 2730 , training loss 55.959358\n",
      "step 2740 , training loss 48.770321\n",
      "step 2750 , training loss 62.234608\n",
      "step 2760 , training loss 72.519577\n",
      "step 2770 , training loss 49.483467\n",
      "step 2780 , training loss 70.159622\n",
      "step 2790 , training loss 69.657112\n",
      "step 2800 , training loss 69.911827\n",
      "step 2810 , training loss 53.307854\n",
      "step 2820 , training loss 69.152733\n",
      "step 2830 , training loss 43.732006\n",
      "step 2840 , training loss 70.157181\n",
      "step 2850 , training loss 38.931770\n",
      "step 2860 , training loss 57.430656\n",
      "step 2870 , training loss 63.711971\n",
      "step 2880 , training loss 66.134285\n",
      "step 2890 , training loss 49.177269\n",
      "step 2900 , training loss 58.486877\n",
      "step 2910 , training loss 45.274574\n",
      "step 2920 , training loss 50.035824\n",
      "step 2930 , training loss 50.985661\n",
      "step 2940 , training loss 63.821663\n",
      "step 2950 , training loss 71.647812\n",
      "step 2960 , training loss 41.764618\n",
      "step 2970 , training loss 60.141548\n",
      "step 2980 , training loss 68.560738\n",
      "step 2990 , training loss 62.500702\n",
      "step 3000 , training loss 43.588768\n",
      "step 3010 , training loss 71.323692\n",
      "step 3020 , training loss 44.210640\n",
      "step 3030 , training loss 70.417084\n",
      "step 3040 , training loss 60.669781\n",
      "step 3050 , training loss 36.233124\n",
      "step 3060 , training loss 57.956326\n",
      "step 3070 , training loss 41.626770\n",
      "step 3080 , training loss 64.719276\n",
      "step 3090 , training loss 55.677917\n",
      "step 3100 , training loss 56.686382\n",
      "step 3110 , training loss 51.154270\n",
      "step 3120 , training loss 59.653198\n",
      "step 3130 , training loss 61.715164\n",
      "step 3140 , training loss 49.452812\n",
      "step 3150 , training loss 35.065491\n",
      "step 3160 , training loss 48.506020\n",
      "step 3170 , training loss 48.427631\n",
      "step 3180 , training loss 56.131660\n",
      "step 3190 , training loss 50.646149\n",
      "step 3200 , training loss 63.369465\n",
      "step 3210 , training loss 63.252739\n",
      "step 3220 , training loss 63.242382\n",
      "step 3230 , training loss 61.087791\n",
      "step 3240 , training loss 50.659431\n",
      "step 3250 , training loss 53.662407\n",
      "step 3260 , training loss 73.011642\n",
      "step 3270 , training loss 56.165489\n",
      "step 3280 , training loss 59.346985\n",
      "step 3290 , training loss 41.662766\n",
      "step 3300 , training loss 58.564686\n",
      "step 3310 , training loss 49.496540\n",
      "step 3320 , training loss 39.213799\n",
      "step 3330 , training loss 49.937836\n",
      "step 3340 , training loss 58.620094\n",
      "step 3350 , training loss 51.011990\n",
      "step 3360 , training loss 46.859962\n",
      "step 3370 , training loss 53.423317\n",
      "step 3380 , training loss 60.869495\n",
      "step 3390 , training loss 62.407387\n",
      "step 3400 , training loss 61.735252\n",
      "step 3410 , training loss 46.818291\n",
      "step 3420 , training loss 58.024242\n",
      "step 3430 , training loss 46.922958\n",
      "step 3440 , training loss 55.779312\n",
      "step 3450 , training loss 56.332695\n",
      "step 3460 , training loss 38.346710\n",
      "step 3470 , training loss 53.611172\n",
      "step 3480 , training loss 56.770874\n",
      "step 3490 , training loss 63.536190\n",
      "step 3500 , training loss 63.158062\n",
      "step 3510 , training loss 60.080368\n",
      "step 3520 , training loss 63.219265\n",
      "step 3530 , training loss 66.881470\n",
      "step 3540 , training loss 48.594398\n",
      "step 3550 , training loss 54.917564\n",
      "step 3560 , training loss 54.963303\n",
      "step 3570 , training loss 50.759399\n",
      "step 3580 , training loss 47.208206\n",
      "step 3590 , training loss 64.491898\n",
      "step 3600 , training loss 55.717880\n",
      "step 3610 , training loss 42.882618\n",
      "step 3620 , training loss 61.229973\n",
      "step 3630 , training loss 51.794312\n",
      "step 3640 , training loss 47.954304\n",
      "step 3650 , training loss 45.235275\n",
      "step 3660 , training loss 42.648079\n",
      "step 3670 , training loss 56.603699\n",
      "step 3680 , training loss 62.613953\n",
      "step 3690 , training loss 47.974792\n",
      "step 3700 , training loss 64.813339\n",
      "step 3710 , training loss 55.607079\n",
      "step 3720 , training loss 70.451271\n",
      "step 3730 , training loss 54.367981\n",
      "step 3740 , training loss 70.576065\n",
      "step 3750 , training loss 61.341106\n",
      "step 3760 , training loss 67.524811\n",
      "step 3770 , training loss 43.360229\n",
      "step 3780 , training loss 49.627617\n",
      "step 3790 , training loss 54.063026\n",
      "step 3800 , training loss 49.336529\n",
      "step 3810 , training loss 47.007641\n",
      "step 3820 , training loss 46.473774\n",
      "step 3830 , training loss 67.034424\n",
      "step 3840 , training loss 71.114380\n",
      "step 3850 , training loss 47.745071\n",
      "step 3860 , training loss 54.507774\n",
      "step 3870 , training loss 54.283371\n",
      "step 3880 , training loss 54.681911\n",
      "step 3890 , training loss 48.963268\n",
      "step 3900 , training loss 59.401398\n",
      "step 3910 , training loss 59.571632\n",
      "step 3920 , training loss 59.300274\n",
      "step 3930 , training loss 54.405727\n",
      "step 3940 , training loss 55.392586\n",
      "step 3950 , training loss 64.167305\n",
      "step 3960 , training loss 56.004761\n",
      "step 3970 , training loss 55.992012\n",
      "step 3980 , training loss 42.951088\n",
      "step 3990 , training loss 54.359264\n",
      "step 4000 , training loss 51.740490\n",
      "step 4010 , training loss 59.045868\n",
      "step 4020 , training loss 54.533623\n",
      "step 4030 , training loss 37.205902\n",
      "step 4040 , training loss 50.342377\n",
      "step 4050 , training loss 43.481178\n",
      "step 4060 , training loss 50.507156\n",
      "step 4070 , training loss 57.024059\n",
      "step 4080 , training loss 57.556026\n",
      "step 4090 , training loss 51.754234\n",
      "step 4100 , training loss 57.089401\n",
      "step 4110 , training loss 61.233246\n",
      "step 4120 , training loss 62.866749\n",
      "step 4130 , training loss 72.070877\n",
      "step 4140 , training loss 57.287067\n",
      "step 4150 , training loss 56.144234\n",
      "step 4160 , training loss 44.056404\n",
      "step 4170 , training loss 46.545433\n",
      "step 4180 , training loss 50.312984\n",
      "step 4190 , training loss 44.868587\n",
      "step 4200 , training loss 55.064461\n",
      "step 4210 , training loss 44.322063\n",
      "step 4220 , training loss 53.704185\n",
      "step 4230 , training loss 64.167702\n",
      "step 4240 , training loss 59.506016\n",
      "step 4250 , training loss 63.477852\n",
      "step 4260 , training loss 59.577751\n",
      "step 4270 , training loss 61.233002\n",
      "step 4280 , training loss 49.810509\n",
      "step 4290 , training loss 64.768204\n",
      "step 4300 , training loss 53.931065\n",
      "step 4310 , training loss 39.141205\n",
      "step 4320 , training loss 55.399384\n",
      "step 4330 , training loss 47.162590\n",
      "step 4340 , training loss 45.629189\n",
      "step 4350 , training loss 47.799778\n",
      "step 4360 , training loss 55.053513\n",
      "step 4370 , training loss 45.458126\n",
      "step 4380 , training loss 52.546883\n",
      "step 4390 , training loss 55.822510\n",
      "step 4400 , training loss 38.656910\n",
      "step 4410 , training loss 50.488564\n",
      "step 4420 , training loss 65.196289\n",
      "step 4430 , training loss 51.208492\n",
      "step 4440 , training loss 57.975163\n",
      "step 4450 , training loss 59.724716\n",
      "step 4460 , training loss 51.287846\n",
      "step 4470 , training loss 62.344780\n",
      "step 4480 , training loss 42.278381\n",
      "step 4490 , training loss 57.518028\n",
      "step 4500 , training loss 62.990414\n",
      "step 4510 , training loss 53.520615\n",
      "step 4520 , training loss 43.491127\n",
      "step 4530 , training loss 60.302551\n",
      "step 4540 , training loss 57.638149\n",
      "step 4550 , training loss 44.715744\n",
      "step 4560 , training loss 60.231125\n",
      "step 4570 , training loss 54.853081\n",
      "step 4580 , training loss 44.513859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4590 , training loss 58.129784\n",
      "step 4600 , training loss 53.641533\n",
      "step 4610 , training loss 39.832153\n",
      "step 4620 , training loss 60.039154\n",
      "step 4630 , training loss 65.603867\n",
      "step 4640 , training loss 55.900826\n",
      "step 4650 , training loss 39.694130\n",
      "step 4660 , training loss 38.759819\n",
      "step 4670 , training loss 52.268997\n",
      "step 4680 , training loss 59.236492\n",
      "step 4690 , training loss 46.182652\n",
      "step 4700 , training loss 40.171631\n",
      "step 4710 , training loss 58.582047\n",
      "step 4720 , training loss 63.444298\n",
      "step 4730 , training loss 53.349178\n",
      "step 4740 , training loss 52.686203\n",
      "step 4750 , training loss 51.612736\n",
      "step 4760 , training loss 40.926849\n",
      "step 4770 , training loss 59.390347\n",
      "step 4780 , training loss 48.495026\n",
      "step 4790 , training loss 49.616570\n",
      "step 4800 , training loss 54.780056\n",
      "step 4810 , training loss 59.636848\n",
      "step 4820 , training loss 51.537842\n",
      "step 4830 , training loss 49.891907\n",
      "step 4840 , training loss 53.925339\n",
      "step 4850 , training loss 47.923439\n",
      "step 4860 , training loss 45.402695\n",
      "step 4870 , training loss 52.452103\n",
      "step 4880 , training loss 56.089336\n",
      "step 4890 , training loss 44.392986\n",
      "step 4900 , training loss 61.363522\n",
      "step 4910 , training loss 53.468994\n",
      "step 4920 , training loss 58.495209\n",
      "step 4930 , training loss 56.553482\n",
      "step 4940 , training loss 66.729805\n",
      "step 4950 , training loss 35.918209\n",
      "step 4960 , training loss 58.972279\n",
      "step 4970 , training loss 52.417297\n",
      "step 4980 , training loss 69.918671\n",
      "step 4990 , training loss 46.797142\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "x = tf.placeholder(tf.float32,[None, 64, 64, 3])\n",
    "y = tf.placeholder(tf.float32, [None])\n",
    "phase = tf.placeholder(tf.bool, [1], name='phase')\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "batch_size = 64\n",
    "\n",
    "with open('/home/cqm/datasets/celeba/index.txt') as input_file:\n",
    "    person_index = []\n",
    "    for line in input_file.readlines():\n",
    "        line = line.strip('\\n').split(\" \")\n",
    "        person_index.append(line[1:])\n",
    "path = '/home/cqm/datasets/celeba/img/'  \n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, w):\n",
    "    return tf.nn.conv2d(x, w, strides = [1, 1, 1, 1], padding = \"SAME\")\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")\n",
    "\n",
    "mybn = tf.contrib.layers.batch_norm\n",
    "def network(x):\n",
    "    w_conv1 = weight_variable([5, 5, 3, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "    h_conv1 = tf.nn.relu(mybn(conv2d(x, w_conv1) + b_conv1, center=True, scale=True, is_training=phase[0]))\n",
    "    h_pool1 = max_pool_2x2(h_conv1) #32*32*32\n",
    "\n",
    "    w_conv2 = weight_variable([3, 3, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "    h_conv2 = tf.nn.relu(mybn(conv2d(h_pool1, w_conv2) + b_conv2, center=True, scale=True, is_training=phase[0]))\n",
    "#     h_pool2 = max_pool_2x2(h_conv2) #64*16*16\n",
    "    w_conv2_2 = weight_variable([3, 3, 64, 64])\n",
    "    b_conv2_2 = bias_variable([64])\n",
    "    h_conv2 = tf.nn.relu(mybn(conv2d(h_conv2, w_conv2_2) + b_conv2_2, center=True, scale=True, is_training=phase[0]))\n",
    "    h_pool2 = max_pool_2x2(h_conv2) #64*16*16\n",
    "\n",
    "    w_conv3 = weight_variable([3, 3, 64, 128])\n",
    "    b_conv3 = bias_variable([128])\n",
    "    h_conv3 = tf.nn.relu(mybn(conv2d(h_pool2, w_conv3) + b_conv3, center=True, scale=True, is_training=phase[0]))\n",
    "#     h_pool3 = max_pool_2x2(h_conv3) #64*8*8\n",
    "    w_conv3_2 = weight_variable([3, 3, 128, 128])\n",
    "    b_conv3_2 = bias_variable([128])\n",
    "    h_conv3 = tf.nn.relu(mybn(conv2d(h_conv3, w_conv3_2) + b_conv3_2, center=True, scale=True, is_training=phase[0]))\n",
    "    h_pool3 = max_pool_2x2(h_conv3) #64*8*8\n",
    "    \n",
    "    w_conv4 = weight_variable([3, 3, 128, 256])\n",
    "    b_conv4 = bias_variable([256])\n",
    "    h_conv4 = tf.nn.relu(mybn(conv2d(h_pool3, w_conv4) + b_conv4, center=True, scale=True, is_training=phase[0]))\n",
    "#     h_pool4 = max_pool_2x2(h_conv4) #128*4*4\n",
    "    w_conv4_2 = weight_variable([3, 3, 256, 256])\n",
    "    b_conv4_2 = bias_variable([256])\n",
    "    h_conv4 = tf.nn.relu(mybn(conv2d(h_conv4, w_conv4_2) + b_conv4_2, center=True, scale=True, is_training=phase[0]))\n",
    "    h_pool4 = max_pool_2x2(h_conv4) #128*4*4\n",
    "    \n",
    "    h_pool4_reshape = tf.reshape(h_pool4, [-1, 4*4*256])\n",
    "\n",
    "    w_fc1 = weight_variable([4*4*256, 512])\n",
    "    b_fc1 = bias_variable([512])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool4_reshape, w_fc1) + b_fc1)\n",
    "    \n",
    "    w_fc2 = weight_variable([512, 512])\n",
    "    h_fc1 = tf.matmul(h_fc1, w_fc2)\n",
    "    #h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    #w_fc2 = weight_variable([512, classnum])\n",
    "    #b_fc2 = bias_variable([classnum])\n",
    "    #y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, w_fc2) +b_fc2)\n",
    "    #y_conv = tf.add(tf.matmul(h_fc1, w_fc2) +b_fc2)\n",
    "    return h_fc1\n",
    "\n",
    "def contrastive_loss(y, d):\n",
    "    tmp = y*tf.square(d)\n",
    "    tmp2 = (1-y)*tf.square(tf.maximum((20-d), 0))\n",
    "    return tf.reduce_sum(tmp+tmp2)/batch_size\n",
    "\n",
    "out = network(x)\n",
    "    \n",
    "d = tf.reduce_sum((out[:batch_size] - out[batch_size:]+1e-5)**2, axis = 1)**0.5\n",
    "loss = contrastive_loss(y, d)\n",
    "train_step = tf.train.AdamOptimizer(2e-5).minimize(loss)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "with open('/home/cqm/datasets/celeba/index.txt') as input_file:\n",
    "    person_index = []\n",
    "    for line in input_file.readlines():\n",
    "        line = line.strip('\\n').split(\" \")\n",
    "        person_index.append(line[1:])\n",
    "\n",
    "for i in range(5000):\n",
    "    pos_pair_list1 = []\n",
    "    pos_pair_list2 = []\n",
    "    while len(pos_pair_list1) < 32:\n",
    "        id = np.random.choice(len(person_index),1,replace=False)[0]\n",
    "        if len(person_index[id]) >= 2:\n",
    "            pair_id = np.random.choice(len(person_index[id]),2,replace=False)\n",
    "            pos_pair_list1.append(person_index[id][pair_id[0]])\n",
    "            pos_pair_list2.append(person_index[id][pair_id[1]])\n",
    "\n",
    "    neg_pair_list1 = []\n",
    "    neg_pair_list2 = []\n",
    "    while len(neg_pair_list1) < 32:\n",
    "        id = np.random.choice(len(person_index),2,replace=False)\n",
    "        neg_id1 = np.random.choice(len(person_index[id[0]]),1,replace=False)[0]\n",
    "        neg_id2 = np.random.choice(len(person_index[id[1]]),1,replace=False)[0]\n",
    "        neg_pair_list1.append(person_index[id[0]][neg_id1])\n",
    "        neg_pair_list2.append(person_index[id[1]][neg_id2])\n",
    "    \n",
    "    x_imgs = []\n",
    "#     y_labels = [1 for num in range(32)]+[0 for num in range(32)]\n",
    "    y_labels = [1] * 32 + [0] * 32\n",
    "    for img_name in pos_pair_list1:\n",
    "        img = cv2.imread(path + img_name)\n",
    "        img = cv2.resize(img, (64, 64))/127.5 - 1.0\n",
    "        x_imgs.append(img)\n",
    "    for img_name in neg_pair_list1:\n",
    "        img = cv2.imread(path + img_name)\n",
    "        img = cv2.resize(img, (64, 64))/127.5 - 1.0\n",
    "        x_imgs.append(img)\n",
    "    for img_name in pos_pair_list2:\n",
    "        img = cv2.imread(path + img_name)\n",
    "        img = cv2.resize(img, (64, 64))/127.5 - 1.0\n",
    "        x_imgs.append(img)\n",
    "    for img_name in neg_pair_list2:\n",
    "        img = cv2.imread(path + img_name)\n",
    "        img = cv2.resize(img, (64, 64))/127.5 - 1.0\n",
    "        x_imgs.append(img)\n",
    "\n",
    "    #print y_labels\n",
    "    x_imgs = np.array(x_imgs, dtype=np.float32)\n",
    "\n",
    "    if i%10 == 0:\n",
    "        train_loss = loss.eval(feed_dict={x:x_imgs,y:y_labels,phase:[True]})\n",
    "        print('step %d , training loss %f'%(i, train_loss))\n",
    "    train_step.run(feed_dict={x:x_imgs,y:y_labels,phase:[True]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = cv2.imread('1/103.jpg')\n",
    "test2 = cv2.imread('1/104.jpg')\n",
    "test3 = cv2.imread('2/203.jpg')\n",
    "test4 = cv2.imread('2/204.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = test1.astype('float32')[None]/127.5 - 1.0\n",
    "test2 = test2.astype('float32')[None]/127.5 - 1.0\n",
    "test3 = test3.astype('float32')[None]/127.5 - 1.0\n",
    "test4 = test4.astype('float32')[None]/127.5 - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = out.eval(feed_dict={x:test1,phase:[False]})\n",
    "res2 = out.eval(feed_dict={x:test2,phase:[False]})\n",
    "res3 = out.eval(feed_dict={x:test3,phase:[False]})\n",
    "res4 = out.eval(feed_dict={x:test4,phase:[False]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.09873e-08\n",
      "4.2052855e-09\n",
      "7.4234423e-09\n",
      "9.656846e-09\n",
      "3.8135582e-08\n",
      "1.8269589e-08\n"
     ]
    }
   ],
   "source": [
    "print(((res3 - res4)**2).sum())\n",
    "print(((res2 - res4)**2).sum())\n",
    "print(((res2 - res3)**2).sum())\n",
    "print(((res1 - res4)**2).sum())\n",
    "print(((res1 - res3)**2).sum())\n",
    "print(((res1 - res2)**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "person1 = []\n",
    "for name in sorted(os.listdir('test_faces/1'))[1:]:\n",
    "    img = cv2.imread('test_faces/1/' + name)\n",
    "    img = cv2.resize(img, (64, 64))/127.5 - 1.0\n",
    "    person1.append(img)\n",
    "person1 = np.array(person1, dtype=np.float32)\n",
    "feature1 = out.eval(feed_dict={x:person1,phase:[False]})\n",
    "\n",
    "person2 = []\n",
    "for name in sorted(os.listdir('test_faces/2'))[1:]:\n",
    "    img = cv2.imread('test_faces/2/' + name)\n",
    "    img = cv2.resize(img, (64, 64))/127.5 - 1.0\n",
    "    person2.append(img)\n",
    "person2 = np.array(person2, dtype=np.float32)\n",
    "feature2 = out.eval(feed_dict={x:person2,phase:[False]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dist(x,y):\n",
    "    return np.sum((x[None] - y[:,None])**2, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0784698e-08"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_dist(feature1,feature1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8749807e-08"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_dist(feature2,feature2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0036566e-08"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_dist(feature1,feature2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'face_rec.ckpt'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver(max_to_keep = 1)\n",
    "saver.save(sess,'face_rec.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.81823998e-05 -1.07714068e-03  1.03330925e-01 -6.87391907e-02\n",
      "  -8.72975588e-02  4.42221090e-02  1.09272506e-02  1.11805275e-01\n",
      "   2.59255506e-02  4.56141382e-02  7.12151080e-02  2.18364239e-01\n",
      "   3.54588591e-02  3.51760685e-02 -1.47523180e-01 -4.73085493e-02\n",
      "  -5.71580231e-03 -2.14989334e-01 -2.83095576e-02 -9.61689427e-02\n",
      "  -1.03918821e-01  3.13162319e-02  1.12282112e-01  1.38177136e-02\n",
      "   8.87697339e-02 -8.75265151e-02  3.59175242e-02 -6.44234046e-02\n",
      "   8.70271865e-03  9.75348875e-02  7.94809610e-02  1.60685807e-01\n",
      "   4.02285345e-02  4.79413122e-02 -1.19370505e-01 -6.82168528e-02\n",
      "   1.20263509e-01 -1.48274601e-01 -6.30757660e-02 -7.11271819e-03\n",
      "   5.57034984e-02  6.97342157e-02 -4.19762991e-02  3.91061455e-02\n",
      "  -1.52945099e-02 -1.05433613e-02 -5.31602129e-02 -4.03903201e-02\n",
      "  -2.83444561e-02  4.51139323e-02 -3.34570371e-02 -1.82932705e-01\n",
      "  -1.14349656e-01 -8.99200095e-05  3.22412979e-03  6.26255646e-02\n",
      "  -3.46180499e-02  1.72794744e-01  1.21809039e-02 -2.28622228e-01\n",
      "  -5.41535532e-03 -4.77418909e-03 -1.50508761e-01  5.44649288e-02\n",
      "   1.61724210e-01  1.74985221e-03 -1.18704669e-01 -7.67651796e-02\n",
      "   5.98183036e-01 -3.27185169e-02  1.62750408e-01 -2.77156327e-02\n",
      "  -1.05481923e-01  6.99482784e-02 -4.11718003e-02  7.94376209e-02\n",
      "   2.86511518e-02 -1.06276624e-01 -2.79641449e-02 -3.86933908e-02\n",
      "  -6.16588593e-02 -5.30883018e-03  1.46446750e-01  1.47651017e-01\n",
      "  -7.29892850e-02 -2.54182424e-02 -1.14872903e-02  1.24070689e-01\n",
      "  -4.20171069e-03  1.39011573e-02  2.49161525e-03  1.37853818e-02\n",
      "  -6.49103075e-02  9.44136828e-02  5.32654673e-02 -5.65816388e-02\n",
      "   2.68179993e-03 -6.34063631e-02  6.67008162e-02  9.39991772e-02\n",
      "  -9.53737050e-02 -4.63357978e-02 -3.99574041e-02  2.61500548e-03\n",
      "  -1.32540077e-01  1.31943390e-01 -9.82775912e-02  1.08887374e-01\n",
      "  -1.11913793e-01 -1.83199927e-01  6.88963244e-03  2.69266907e-02\n",
      "   5.93907721e-02 -5.50599247e-02  1.11943379e-01 -2.53329258e-02\n",
      "   6.62294850e-02 -9.36198086e-02  1.62852064e-01 -6.36808276e-02\n",
      "  -8.81167427e-02  1.34162560e-01 -6.15974031e-02 -1.98887810e-02\n",
      "   1.20507181e-02 -5.62275425e-02 -4.77827862e-02 -1.07847877e-01\n",
      "  -4.76021841e-02  6.20675925e-03  1.14649713e-01 -5.02638631e-02\n",
      "  -4.97101620e-03  1.61855370e-01 -7.62922689e-02 -3.24636027e-02\n",
      "   4.15239856e-03  2.12407224e-02 -1.18794218e-01 -5.66193648e-03\n",
      "   3.44956815e-02  8.43429118e-02 -1.31088033e-01  4.62123156e-02\n",
      "   8.44017789e-02 -3.79429571e-02 -4.97092912e-03  4.24439833e-03\n",
      "  -1.34973615e-01 -1.83801845e-01  2.21380651e-01  3.24803628e-02\n",
      "   9.09425505e-03 -2.14207709e-01 -2.16748789e-02  2.05876008e-02\n",
      "   3.86683196e-02 -3.29426043e-02 -8.80539864e-02  1.11241790e-03\n",
      "   7.38330707e-02  6.84336722e-02 -1.17465153e-01 -1.18243620e-01\n",
      "  -1.64854407e-01 -1.79220270e-02 -8.01845267e-02 -2.33883619e-01\n",
      "   3.15108858e-02 -1.41255241e-02  4.22138497e-02  5.58941364e-02\n",
      "   9.42420736e-02 -1.72749963e-02  5.69805093e-02 -7.57416338e-02\n",
      "  -1.40440807e-01  8.28537643e-02  7.75430128e-02  1.04136191e-01\n",
      "  -1.75843239e-01  1.36696501e-02  3.66497524e-02 -4.76245396e-02\n",
      "   1.00762866e-01 -4.92161512e-02 -7.78152049e-02  3.54705267e-02\n",
      "   8.41439217e-02  3.83816883e-02 -8.77405405e-02  2.26464093e-01\n",
      "  -9.28585604e-02  8.12639222e-02  4.81957532e-02 -4.20318432e-02\n",
      "  -9.23554506e-03  2.57360209e-02  1.62232183e-02 -1.82370052e-01\n",
      "   6.25589956e-03 -6.01085871e-02 -8.39362741e-02  7.87524134e-03\n",
      "   1.56872839e-01  5.14667556e-02 -2.80678291e-02 -1.14026638e-02\n",
      "   1.75698623e-02 -1.11266896e-02  2.50752438e-02 -5.03284968e-02\n",
      "  -2.42509376e-02 -1.23633593e-01 -5.72977066e-02 -7.23323748e-02\n",
      "  -1.65448487e-01  1.63026854e-01  2.66117454e-02  8.43365490e-03\n",
      "   3.82431522e-02  4.01236713e-02 -1.10997416e-01 -2.50512511e-02\n",
      "   8.89234915e-02 -1.61069557e-02  1.49098694e-01 -7.61559680e-02\n",
      "  -1.45362848e-02  1.69106908e-02  1.26689225e-02 -4.54135165e-02\n",
      "  -1.54326662e-01 -1.22456532e-02 -7.17288032e-02  2.07524002e-02\n",
      "  -6.48666397e-02  1.22064389e-01 -9.87946391e-02 -2.20890995e-02\n",
      "   9.09819081e-02 -4.36790474e-02  1.05889879e-01 -8.90303310e-03\n",
      "  -4.91868220e-02  2.37845797e-02 -3.24855670e-02 -9.22091678e-02\n",
      "  -4.42553572e-02 -6.71978965e-02 -6.97752237e-02 -1.01423889e-01\n",
      "   1.60348061e-02  1.19937034e-02  1.22424632e-01 -9.15272981e-02\n",
      "   2.58194152e-02 -1.64180160e-01 -1.03009582e-01 -1.00811005e-01\n",
      "  -4.67711873e-03  1.23626813e-01  9.93861556e-02  6.36229366e-02\n",
      "  -1.42375141e-01 -3.41287926e-02  7.04729185e-03 -1.18672140e-01\n",
      "   7.86885545e-02  9.35558975e-02 -1.61109567e-02  7.33822733e-02\n",
      "   3.17880847e-02  1.15613759e-01 -3.63556482e-02  1.16400952e-04\n",
      "  -6.28670305e-02 -1.11286819e-01 -1.33736908e-01 -2.63186321e-02\n",
      "  -3.87584642e-02 -5.43782488e-02  3.68695408e-02 -3.99753824e-02\n",
      "   1.56270321e-02  1.64866485e-02  1.59595370e-01  1.76968008e-01\n",
      "  -8.60960968e-03 -3.51142846e-02  2.70489361e-02  9.55644101e-02\n",
      "  -8.19986803e-04 -5.86824752e-02  5.00416895e-03  1.10297143e-01\n",
      "  -8.95058960e-02 -3.24932523e-02 -1.62678733e-01 -3.18813473e-02\n",
      "   2.09497198e-01 -6.54835179e-02 -2.89016888e-02 -1.69539034e-01\n",
      "  -4.45667915e-02  1.39022440e-01 -3.98367345e-02 -2.10746303e-01\n",
      "  -4.37656753e-02  4.58798483e-02 -1.32866809e-03  5.88329844e-02\n",
      "  -9.02909562e-02 -1.37447119e-01  5.14350925e-03 -1.78322792e-01\n",
      "  -1.93436742e-01 -1.15811161e-03  3.34456936e-02  4.61509675e-02\n",
      "  -2.36682761e-02 -8.51086974e-02 -2.04856712e-02 -6.62440434e-02\n",
      "  -1.14889359e-02 -7.89604783e-02 -1.01192665e-04  1.90057725e-01\n",
      "   1.21507011e-01  1.24455631e-01  4.16775793e-02 -7.28820041e-02\n",
      "  -1.77368000e-02  5.82156181e-02  1.65686645e-02 -1.22585237e-01\n",
      "  -6.87079411e-03 -1.23538449e-01  2.95217969e-02 -1.33476198e-01\n",
      "  -6.95387870e-02  5.12940697e-02  2.53960676e-02 -1.04585402e-01\n",
      "  -6.50002360e-02 -7.50180334e-02  8.08833838e-02  5.09997457e-02\n",
      "  -1.54431704e-02  1.46433890e-01 -3.91996233e-03  8.40369016e-02\n",
      "   4.92498204e-02 -6.14093281e-02  1.82924103e-02 -6.20497018e-02\n",
      "  -2.93678641e-02 -3.97622921e-02 -5.12809642e-02 -4.91707362e-02\n",
      "  -7.59045184e-02  8.65992084e-02  1.77591592e-01  3.94854769e-02\n",
      "   4.73337322e-02 -7.18436241e-02  8.63928124e-02  6.13985546e-02\n",
      "   1.41858803e-02 -2.13579714e-01 -1.87232539e-01 -9.46944505e-02\n",
      "   9.72961448e-03  3.49106900e-02 -6.05518259e-02 -9.88904238e-02\n",
      "   4.62618889e-03  2.40491442e-02 -1.06815748e-01  9.06196162e-02\n",
      "   8.48402232e-02  4.27541044e-03 -2.47350410e-02  3.55730243e-02\n",
      "  -7.45113194e-03  2.66976506e-02 -7.94785768e-02 -7.69169331e-02\n",
      "  -1.17327660e-01  4.22898605e-02 -1.55270323e-01 -3.68027836e-02\n",
      "  -8.25959444e-02 -9.87470075e-02  3.34997326e-02 -1.04160711e-01\n",
      "   7.01689795e-02  2.56431885e-02 -4.79661226e-02 -2.02801540e-01\n",
      "   8.03562552e-02 -7.70312920e-02 -7.51965865e-03 -7.53704272e-03\n",
      "  -4.41553816e-02 -6.96623325e-03  7.83196464e-02 -4.05814983e-02\n",
      "   2.83259572e-03  1.14744209e-01  2.42401306e-02 -1.69994324e-01\n",
      "   1.08393371e-01 -2.38500368e-02 -5.71639910e-02  3.33254151e-02\n",
      "  -9.99315456e-02 -7.77072506e-03  9.96853560e-02 -4.60454915e-03\n",
      "  -1.58097669e-02 -8.60529616e-02 -5.94434775e-02  7.62506500e-02\n",
      "  -3.04465108e-02  5.37223332e-02 -5.06685004e-02  1.02263018e-01\n",
      "  -7.10015073e-02  2.14507967e-01  8.69267061e-02  7.15003069e-03\n",
      "  -4.04690430e-02  5.28751686e-02  3.47802602e-02  4.46512625e-02\n",
      "   4.02269922e-02  8.63317996e-02 -3.70545685e-02  5.70179336e-02\n",
      "   1.46461613e-02 -5.43902777e-02 -1.81319624e-01 -8.44779462e-02\n",
      "  -3.62361707e-02 -1.35850124e-02  3.23018581e-02 -2.03777254e-02\n",
      "  -1.75391845e-02 -8.10335800e-02  3.53268497e-02 -2.79205590e-01\n",
      "   8.12930837e-02  1.52825847e-01 -3.22334245e-02 -5.67241237e-02\n",
      "   4.37260866e-02 -9.98053551e-02  8.11218768e-02  1.57929454e-02\n",
      "  -3.18679139e-02 -1.33727444e-04 -4.56496933e-03 -4.08276245e-02\n",
      "  -1.57075271e-01 -1.34842604e-01  2.20668837e-02 -8.51073787e-02\n",
      "  -5.82708903e-02 -1.66962907e-01 -1.17244475e-01 -1.25714034e-01\n",
      "  -7.26840720e-02  1.47092212e-02  6.81234375e-02 -8.72432590e-02\n",
      "  -8.70928615e-02 -2.32405625e-02  1.73820648e-02  9.06421542e-02\n",
      "  -6.83505535e-02  1.92856397e-02 -3.47490087e-02  9.39989369e-03\n",
      "  -9.49718803e-02  1.04281709e-01  4.44650911e-02  4.82707694e-02\n",
      "  -1.06312651e-02 -1.32498771e-01 -7.23255351e-02  5.01782000e-02\n",
      "   8.58529285e-03  3.69536132e-02 -5.46613857e-02  1.71386898e-01\n",
      "  -2.69964598e-02  1.17815509e-02  9.57016051e-02  8.31153803e-03\n",
      "  -1.74898699e-01  1.52957052e-01  1.18292905e-01 -7.19935587e-03\n",
      "  -1.38339713e-01  5.02913408e-02  9.53528739e-04  2.25664079e-02\n",
      "   7.79842064e-02  6.33199587e-02  2.29419231e-01  4.18474041e-02]]\n"
     ]
    }
   ],
   "source": [
    "print res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
